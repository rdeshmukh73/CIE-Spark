{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f568c68",
   "metadata": {},
   "source": [
    "### Separate prompt evaluation for each criterion (Alternative Implementation)\n",
    "This notebook uses independent API calls for each evaluation criterion without maintaining chat context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53582d",
   "metadata": {},
   "source": [
    "### Key Implementation Details\n",
    "\n",
    "**Architecture:**\n",
    "- Each evaluation criterion gets a separate, independent API call\n",
    "- No context is maintained between different criteria evaluations\n",
    "- Each call creates a fresh chat session\n",
    "\n",
    "**Rate Limiting Strategy:**\n",
    "1. Configurable delay between each API call (default: 1s)\n",
    "2. Additional delay between processing different teams (default: 2s)\n",
    "3. Retry logic with backoff for failed requests\n",
    "4. Progress tracking with detailed logging\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Resilient**: If one criterion fails, others continue\n",
    "- ‚úÖ **Transparent**: Clear progress tracking and error reporting\n",
    "- ‚úÖ **Configurable**: Easy to adjust rate limits based on API quotas\n",
    "- ‚úÖ **Scalable**: Can be extended to parallel processing\n",
    "\n",
    "**Trade-offs:**\n",
    "- ‚ö†Ô∏è May use more tokens than multi-turn approach (no context sharing)\n",
    "- ‚ö†Ô∏è Takes longer due to rate limiting delays\n",
    "- ‚ö†Ô∏è No semantic connection between criterion evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6557df43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai import chats\n",
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43681a29",
   "metadata": {},
   "source": [
    "### TIPSC Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb402024",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIPSC_FEW_SHOT_EXAMPLES = f\"\"\"\n",
    "Example 1 - \n",
    "Pitch Statement:\n",
    "\"An AI-powered tool that detects early signs of diabetic foot ulcers using smartphone images, helping rural healthcare \n",
    "workers intervene before complications arise.\"\n",
    "\n",
    "TIPSC Review:\n",
    "Timely: Rising diabetes cases in rural areas make early detection critical. (Score: 5)\n",
    "Important: Addresses a major healthcare gap affecting millions. (Score: 5)\n",
    "Profitable: Strong market through health-tech startups and public health programs. (Score: 4)\n",
    "Solvable: Feasible with current AI imaging and mobile tech. (Score: 5)\n",
    "Contextual: Team has medical + AI expertise with NGO partnerships. (Score: 5)\n",
    "\n",
    "Overall Assessment: Excellent (95%)\n",
    "Brief Justification: The problem is urgent, large-scale, and solvable with current technology. \n",
    "The team‚Äôs alignment with healthcare stakeholders strengthens contextual fit and market potential.\n",
    "\n",
    "Example 2 -\n",
    "Pitch Statement:\n",
    "\"A wearable hydration tracker that reminds users to drink water based on real-time sweat analysis and weather conditions.\"\n",
    "\n",
    "TIPSC Review:\n",
    "Timely: Wellness tech is growing, though hydration-specific solutions are not urgent. (Score: 4)\n",
    "Important: Moderate market among fitness and sports users. (Score: 4)\n",
    "Profitable: Viable as a premium product but niche appeal. (Score: 4)\n",
    "Solvable: Current sensors and IoT make it achievable. (Score: 5)\n",
    "Contextual: Team has IoT experience but limited market understanding. (Score: 3)\n",
    "\n",
    "Overall Assessment: Good (80%)\n",
    "Brief Justification: A relevant and buildable solution with moderate market potential; success depends on positioning \n",
    "and user adoption beyond enthusiasts.\n",
    "\n",
    "Example 3 - \n",
    "Pitch Statement:\n",
    "\"A desktop app to remind remote workers to stretch every 30 minutes and suggest exercises.\"\n",
    "\n",
    "TIPSC Review:\n",
    "Timely: The post-pandemic remote work trend is stabilizing. (Score: 3)\n",
    "Important: Mildly useful but low perceived urgency. (Score: 3)\n",
    "Profitable: Free alternatives exist; monetization unclear. (Score: 2)\n",
    "Solvable: Technically simple; easy to build. (Score: 5)\n",
    "Contextual: Team has coding skills but lacks health/UX expertise. (Score: 3)\n",
    "\n",
    "Overall Assessment: Fair (60%)\n",
    "Brief Justification: Simple, achievable idea with limited novelty and unclear market traction; \n",
    "lacks compelling urgency or differentiator.\n",
    "\n",
    "Example 4 - \n",
    "Pitch Statement:\n",
    "\"An app that plays motivational quotes every hour to keep users positive.\"\n",
    "\n",
    "TIPSC Review:\n",
    "\n",
    "Timely: No clear trend or urgency for hourly motivational quotes. (Score: 2)\n",
    "Important: Trivial problem with low impact. (Score: 2)\n",
    "Profitable: Difficult to monetize; saturated with free apps. (Score: 1)\n",
    "Solvable: Technically easy but adds little value. (Score: 4)\n",
    "Contextual: Team lacks psychological or design expertise. (Score: 2)\n",
    "\n",
    "Overall Assessment: Poor (40%)\n",
    "Brief Justification: While easily implementable, the idea solves no pressing problem, \n",
    "lacks clear market differentiation, and shows weak contextual relevance.\n",
    "\n",
    "Example 5 -\n",
    "Pitch Statement:\n",
    "\"A low-cost smart inhaler system that tracks asthma medication usage, predicts attacks using environmental data, \n",
    "and alerts caregivers in real time.\"\n",
    "\n",
    "TIPSC Review:\n",
    "\n",
    "Timely: Asthma rates are increasing due to urban pollution; immediate relevance. (Score: 5)\n",
    "Important: Critical for patients, families, and healthcare providers. (Score: 5)\n",
    "Profitable: Strong potential for insurance tie-ins and health partnerships. (Score: 4)\n",
    "Solvable: Current IoT + predictive AI make this feasible. (Score: 5)\n",
    "Contextual: Team has biomedical and data analytics background. (Score: 5)\n",
    "\n",
    "Overall Assessment: Excellent (95%)\n",
    "Brief Justification: Urgent and impactful healthcare problem with a clear path to implementation and adoption; \n",
    "strong interdisciplinary team fit enhances feasibility and trust.\n",
    "\n",
    "Example 6 - \n",
    "Pitch Statement:\n",
    "\"An AI chatbot that suggests eco-friendly alternatives when users shop online ‚Äî like showing sustainable brands or \n",
    "second-hand options.\"\n",
    "\n",
    "TIPSC Review:\n",
    "\n",
    "Timely: Sustainability awareness is increasing but not yet mainstream behavior. (Score: 4)\n",
    "Important: Appeals to a growing but niche eco-conscious segment. (Score: 4)\n",
    "Profitable: Monetization possible via affiliate or brand partnerships. (Score: 4)\n",
    "Solvable: Readily achievable using APIs and recommendation engines. (Score: 5)\n",
    "Contextual: Team has AI experience but limited marketing background. (Score: 3)\n",
    "\n",
    "Overall Assessment: Good (80%)\n",
    "Brief Justification: Strong alignment with sustainability trends and implementable tech; \n",
    "moderate commercial potential limited by user behavior change barriers.\n",
    "\n",
    "Example 7 -\n",
    "Pitch Statement:\n",
    "\"A mobile app that helps people organize their daily to-do lists using colorful emojis and sound alerts to make productivity fun.\"\n",
    "\n",
    "TIPSC Review:\n",
    "\n",
    "Timely: Productivity apps remain evergreen but oversaturated. (Score: 3)\n",
    "Important: Low differentiation; helps individuals but no major impact. (Score: 3)\n",
    "Profitable: Hard to stand out in a crowded, free-app market. (Score: 2)\n",
    "Solvable: Simple app; easily buildable with existing frameworks. (Score: 5)\n",
    "Contextual: Team has beginner-level coding skills; limited UX experience. (Score: 3)\n",
    "\n",
    "Overall Assessment: Fair (60%)\n",
    "Brief Justification: Technically achievable but lacks novelty, urgency, and clear market pull; \n",
    "execution quality will determine limited success.\n",
    "\n",
    "Example 8 -\n",
    "Pitch Statement:\n",
    "\"An app that changes your phone wallpaper every hour to keep you inspired and motivated throughout the day.\"\n",
    "\n",
    "TIPSC Review:\n",
    "\n",
    "Timely: No identifiable need or trend driving this idea. (Score: 2)\n",
    "Important: Minimal user impact; cosmetic value only. (Score: 2)\n",
    "Profitable: No clear revenue stream or differentiator. (Score: 1)\n",
    "Solvable: Very easy to build with existing APIs. (Score: 4)\n",
    "Contextual: Team lacks direction and product reasoning. (Score: 2)\n",
    "\n",
    "Overall Assessment: Poor (40%)\n",
    "Brief Justification: Technically trivial concept with no significant need, value proposition, \n",
    "or sustainable market advantage; fails to meet hackathon impact criteria.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307c3cd",
   "metadata": {},
   "source": [
    "### Problem Evidence and Validation (Weightage : 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea2912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_prob_evidence_val(problem_statement_text) :\n",
    "    return f\"\"\"\n",
    "        You are an expert evaluator for university hackathon pitch decks. Your task is to assess the Problem Evidence & Validation based on the rubric below.\n",
    "\n",
    "        RUBRIC:\n",
    "        - Excellent (90-100%): 10+ interviews with diverse stakeholders; multiple direct quotes; clear quantification of time/money impact\n",
    "        - Good (70-89%): 5-9 interviews; some relevant quotes; basic quantification\n",
    "        - Fair (50-69%): 3-4 interviews; limited evidence; vague numbers\n",
    "        - Poor (0-49%): <3 interviews; no direct evidence; purely anecdotal\n",
    "\n",
    "        The Problem Evidence and Validation content to evaluate is here:  {problem_statement_text}\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        1. Assign ONE category: Excellent, Good, Fair, or Poor\n",
    "        2. Provide a 2-3 sentence justification citing specific evidence (or lack thereof) from the pitch deck\n",
    "        3. Note the approximate number of interviews mentioned (if any)\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        Category: [Excellent/Good/Fair/Poor]\n",
    "        Justification: [Your short 2-3 sentence reasoning]\n",
    "        Interview Count: [Number or \"Not specified\"]\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5359273",
   "metadata": {},
   "source": [
    "### Market Opportunity & Viability (Weightage : 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bacd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_market_viability(market_opportunity_viability_text) :\n",
    "   return f\"\"\"\n",
    "      You are an expert evaluator for university hackathon pitch decks. Your task is to assess Market Opportunity & Viability \n",
    "      based on the rubric below.\n",
    "\n",
    "      RUBRIC:\n",
    "      - Excellent (90-100%): Clear TAM/SAM/SOM with credible sources; strong profitability argument; competitive gap identified\n",
    "      - Good (70-89%): Basic market sizing; some business potential; mentions competitors\n",
    "      - Fair (50-69%): Vague market references; unclear business model\n",
    "      - Poor (0-49%): No market analysis; no commercial viability\n",
    "\n",
    "      The Market Opportunity and Viability content to evaluate is here: {market_opportunity_viability_text}\n",
    "\n",
    "      INSTRUCTIONS:\n",
    "      1. Assign ONE category: Excellent, Good, Fair, or Poor\n",
    "      2. Provide a 2-3 sentence justification focusing on:\n",
    "         - Quality of market sizing (TAM/SAM/SOM presence and credibility)\n",
    "         - Business model clarity\n",
    "         - Competitive analysis depth\n",
    "      3. Note if credible sources are cited for market data\n",
    "\n",
    "      OUTPUT FORMAT:\n",
    "      Category: [Excellent/Good/Fair/Poor]\n",
    "      Justification: [Your short 2-3 sentence reasoning]\n",
    "      Market Data Quality: [Strong/Moderate/Weak/Absent]\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9389c1",
   "metadata": {},
   "source": [
    "### TIPSC (Weightage : 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddf7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_tipsc(tipsc_text) :\n",
    "   return f\"\"\"\n",
    "      You are an expert evaluator for university hackathon pitch decks. Your task is to assess Problem Significance using \n",
    "      the TIPSC framework. \n",
    "      TIPSC means the following:\n",
    "      T = Timely = Is the problem curent and in need of an urgent solution or recently emergent and a solution can wait?\n",
    "      I = Important = Does the solution or solving this problem matter to a large or key group of customers or market sectors/segments?\n",
    "      P = Profitable = Will solving this problem yield Revenue or Value or a potential for these exist (even if limited)?\n",
    "      S = Solvable = Is it possible to create a solution for this problem now given the technology and other required resources?\n",
    "      C = Contextual = Is the current situation like team, policiefs, company, approach the right fit?\n",
    "\n",
    "      Here are a few examples of how to evaluate or assess TIPSC: \n",
    "      {TIPSC_FEW_SHOT_EXAMPLES}\n",
    "\n",
    "      RUBRIC:\n",
    "      - Excellent (90-100%): Compelling urgency + major impact + clear team advantage + realistic solution path\n",
    "      - Good (70-89%): Some timeliness + moderate impact + reasonable team fit\n",
    "      - Fair (50-69%): Vague timing + minor impact + generic team fit\n",
    "      - Poor (0-49%): No urgency + trivial problem + poor team fit\n",
    "\n",
    "      The TIPSC Content to be evaluated is: {tipsc_text}\n",
    "\n",
    "      INSTRUCTIONS:\n",
    "      1. Assign ONE category: Excellent, Good, Fair, or Poor\n",
    "      2. Provide a 2-3 sentence justification addressing:\n",
    "         - Timeliness/urgency of the problem\n",
    "         - Scale and severity of impact\n",
    "         - Team's relevant advantage or expertise\n",
    "         - Realism of proposed solution path\n",
    "      3. Identify the strongest and weakest TIPSC element\n",
    "\n",
    "      OUTPUT FORMAT:\n",
    "      Category: [Excellent/Good/Fair/Poor]\n",
    "      Justification: [Your reasoning]\n",
    "      Strongest Element: [T/I/P/S/C]\n",
    "      Weakest Element: [T/I/P/S/C]\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df31f8",
   "metadata": {},
   "source": [
    "### Solution Direction & Value Proposition (Weightage : 15%)\n",
    "### FOR RD SIR TO VERIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1172ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_solution(solution_value_prop) :\n",
    "   return f\"\"\"\n",
    "      You are an expert evaluator for university hackathon pitch decks. Your task is to assess Solution Direction & Value Proposition based on the rubric below.\n",
    "\n",
    "      RUBRIC:\n",
    "      - Excellent (90-100%): Clear solution hypothesis directly addressing gaps; strong unique value proposition\n",
    "      - Good (70-89%): Basic solution direction; addresses some gaps\n",
    "      - Fair (50-69%): Vague solution idea; weak value proposition\n",
    "      - Poor (0-49%): No clear solution direction; copies existing solutions\n",
    "\n",
    "      Solution Hypothesis of the Pitch Deck is here: {solution_value_prop}\n",
    "\n",
    "      INSTRUCTIONS:\n",
    "      1. Assign ONE category: Excellent, Good, Fair, or Poor\n",
    "      2. Provide a 2-3 sentence justification focusing on:\n",
    "         - Strength of value proposition\n",
    "         - Real-world impact\n",
    "      3. Note the most significant presentation strength or weakness\n",
    "\n",
    "      OUTPUT FORMAT:\n",
    "      Category: [Excellent/Good/Fair/Poor]\n",
    "      Justification: [Your reasoning]\n",
    "      Key Strength/Weakness: [Brief description]\n",
    "\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fb758",
   "metadata": {},
   "source": [
    "### Presentation Comprehension (Weightage : 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d4e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_pres_comp(presentation_cohesion) :\n",
    "   return f\"\"\"\n",
    "      You are an expert evaluator for university hackathon pitch decks. Your task is to assess Presentation & Cohesion based on the rubric below.\n",
    "\n",
    "      RUBRIC:\n",
    "      - Excellent (90-100%): Compelling narrative; logical flow; professional design; clear communication\n",
    "      - Good (70-89%): Mostly coherent; decent design; some gaps in logic\n",
    "      - Fair (50-69%): Disjointed arguments; basic design; confusing flow\n",
    "      - Poor (0-49%): Incoherent story; poor design; unclear messaging\n",
    "\n",
    "      Summary of the Pitch Deck is here: {presentation_cohesion}\n",
    "\n",
    "\n",
    "      INSTRUCTIONS:\n",
    "      1. Assign ONE category: Excellent, Good, Fair, or Poor\n",
    "      2. Provide a 2-3 sentence justification focusing on:\n",
    "         - Narrative coherence and logical flow\n",
    "         - Clarity of communication\n",
    "         - Overall professional quality\n",
    "      3. Note the most significant presentation strength or weakness\n",
    "\n",
    "      OUTPUT FORMAT:\n",
    "      Category: [Excellent/Good/Fair/Poor]\n",
    "      Justification: [Your reasoning]\n",
    "      Key Strength/Weakness: [Brief description]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66de31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_decks_df = pd.read_csv(\"../EvaluateStudentIdeas/pitch_decks_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5be3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to replaces spaces with underscores\n",
    "for col in pitch_decks_df :\n",
    "    pitch_decks_df = pitch_decks_df.rename(columns={col : col.replace(' ', '_')})\n",
    "pitch_decks_df = pitch_decks_df.rename(columns={'Problem_Statement_(cleaned)' : 'Problem_Statement_Cleaned'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2abc027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_Name</th>\n",
       "      <th>Problem_Statement</th>\n",
       "      <th>Problem_Evidence</th>\n",
       "      <th>Market_Opportunity_Viability</th>\n",
       "      <th>TIPSC</th>\n",
       "      <th>Competition</th>\n",
       "      <th>Solution_Hypothesis</th>\n",
       "      <th>References</th>\n",
       "      <th>Problem_Statement_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaSmart Innovations</td>\n",
       "      <td>Slide 1: The Problem &amp; The Team - Team Name: A...</td>\n",
       "      <td>Slide 2: Evidence of Customer's Pain Point - K...</td>\n",
       "      <td>Slide 3: Quantifying the Problem - Market Size...</td>\n",
       "      <td>Slide 4: Why This Problem is TIPSC - Timely: C...</td>\n",
       "      <td>Slide 5: Competitive Landscape &amp; The Gap - Cur...</td>\n",
       "      <td>Slide 6: Solution Hypothesis - Proposed Soluti...</td>\n",
       "      <td>Slide 7: Next Steps - Prototype testing in 500...</td>\n",
       "      <td>Core Problem Statement: Urban Water Conservati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...</td>\n",
       "      <td>Slide 1: The Problem &amp; The Team Team Name: Tri...</td>\n",
       "      <td>Slide 2: Evidence of Customer‚Äôs Pain Point R&amp;D...</td>\n",
       "      <td>Slide 3: Quantifying the Problem Market Size (...</td>\n",
       "      <td>Slide 4: Why This Problem is TIPSC (The Strate...</td>\n",
       "      <td>Slide 5: The Competitive Landscape &amp; The Gap C...</td>\n",
       "      <td>Slide 6: The Solution Hypothesis (High-Level O...</td>\n",
       "      <td>Slide 7: Appendix, References &amp; Next Steps Our...</td>\n",
       "      <td>Core Problem Statement: Artisan Market Access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AgriSat Tech</td>\n",
       "      <td>Slide 1: The Problem &amp; The Team - Team Name: A...</td>\n",
       "      <td>Slide 2: Evidence of Customer's Pain Point - K...</td>\n",
       "      <td>Slide 3: Quantifying the Problem - Market Size...</td>\n",
       "      <td>Slide 4: Why This Problem is TIPSC - Timely: I...</td>\n",
       "      <td>Slide 5: Competitive Landscape &amp; The Gap - Cur...</td>\n",
       "      <td>Slide 6: Solution Hypothesis - Proposed Soluti...</td>\n",
       "      <td>Slide 7: Next Steps - Partner with ISRO for sa...</td>\n",
       "      <td>Core Problem Statement: Precision Agriculture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RuralConnect</td>\n",
       "      <td>Slide 1: The Problem &amp; The Team - Team Name: R...</td>\n",
       "      <td>Slide 2: Evidence of Customer's Pain Point - K...</td>\n",
       "      <td>Slide 3: Quantifying the Problem - Market Size...</td>\n",
       "      <td>Slide 4: Why This Problem is TIPSC - Timely: D...</td>\n",
       "      <td>Slide 5: Competitive Landscape &amp; The Gap - Cur...</td>\n",
       "      <td>Slide 6: Solution Hypothesis - Proposed Soluti...</td>\n",
       "      <td>Slide 7: Next Steps - Pilot in 50 villages acr...</td>\n",
       "      <td>Core Problem Statement: - Last Mile Rural Conn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Team_Name  \\\n",
       "0                              AquaSmart Innovations   \n",
       "1  Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...   \n",
       "2                                       AgriSat Tech   \n",
       "3                                       RuralConnect   \n",
       "\n",
       "                                   Problem_Statement  \\\n",
       "0  Slide 1: The Problem & The Team - Team Name: A...   \n",
       "1  Slide 1: The Problem & The Team Team Name: Tri...   \n",
       "2  Slide 1: The Problem & The Team - Team Name: A...   \n",
       "3  Slide 1: The Problem & The Team - Team Name: R...   \n",
       "\n",
       "                                    Problem_Evidence  \\\n",
       "0  Slide 2: Evidence of Customer's Pain Point - K...   \n",
       "1  Slide 2: Evidence of Customer‚Äôs Pain Point R&D...   \n",
       "2  Slide 2: Evidence of Customer's Pain Point - K...   \n",
       "3  Slide 2: Evidence of Customer's Pain Point - K...   \n",
       "\n",
       "                        Market_Opportunity_Viability  \\\n",
       "0  Slide 3: Quantifying the Problem - Market Size...   \n",
       "1  Slide 3: Quantifying the Problem Market Size (...   \n",
       "2  Slide 3: Quantifying the Problem - Market Size...   \n",
       "3  Slide 3: Quantifying the Problem - Market Size...   \n",
       "\n",
       "                                               TIPSC  \\\n",
       "0  Slide 4: Why This Problem is TIPSC - Timely: C...   \n",
       "1  Slide 4: Why This Problem is TIPSC (The Strate...   \n",
       "2  Slide 4: Why This Problem is TIPSC - Timely: I...   \n",
       "3  Slide 4: Why This Problem is TIPSC - Timely: D...   \n",
       "\n",
       "                                         Competition  \\\n",
       "0  Slide 5: Competitive Landscape & The Gap - Cur...   \n",
       "1  Slide 5: The Competitive Landscape & The Gap C...   \n",
       "2  Slide 5: Competitive Landscape & The Gap - Cur...   \n",
       "3  Slide 5: Competitive Landscape & The Gap - Cur...   \n",
       "\n",
       "                                 Solution_Hypothesis  \\\n",
       "0  Slide 6: Solution Hypothesis - Proposed Soluti...   \n",
       "1  Slide 6: The Solution Hypothesis (High-Level O...   \n",
       "2  Slide 6: Solution Hypothesis - Proposed Soluti...   \n",
       "3  Slide 6: Solution Hypothesis - Proposed Soluti...   \n",
       "\n",
       "                                          References  \\\n",
       "0  Slide 7: Next Steps - Prototype testing in 500...   \n",
       "1  Slide 7: Appendix, References & Next Steps Our...   \n",
       "2  Slide 7: Next Steps - Partner with ISRO for sa...   \n",
       "3  Slide 7: Next Steps - Pilot in 50 villages acr...   \n",
       "\n",
       "                           Problem_Statement_Cleaned  \n",
       "0  Core Problem Statement: Urban Water Conservati...  \n",
       "1  Core Problem Statement: Artisan Market Access ...  \n",
       "2  Core Problem Statement: Precision Agriculture ...  \n",
       "3  Core Problem Statement: - Last Mile Rural Conn...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_decks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48ae72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key = os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6b1cf",
   "metadata": {},
   "source": [
    "### Rate Limiting Configuration\n",
    "Adjust these parameters based on your API quota and rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5027692",
   "metadata": {},
   "source": [
    "### LLM-evaluation using separate prompts for each criterion\n",
    "**Key differences from multi-turn approach:**\n",
    "- Each criterion uses an independent API call\n",
    "- No context sharing between evaluations\n",
    "- Rate limiting to avoid API throttling\n",
    "- Progress tracking for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5997fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_cols = ['Team_Name', 'Problem_Evidence', 'Market_Opp_Viability', 'TIPSC', 'Solution_Dir_Val_Prop', 'Pres_Cohesion', 'Final_Score']\n",
    "grade_df = pd.DataFrame(columns=grade_cols)\n",
    "\n",
    "token_cols = ['Team_Name', 'Candidate_Tokens', 'Thought_Tokens', 'Input_Tokens', 'Output_Tokens', 'Total_Tokens']\n",
    "token_df = pd.DataFrame(columns=token_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ef77439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for rate limiting\n",
    "DELAY_BETWEEN_CALLS = 1.0  # seconds between API calls\n",
    "DELAY_BETWEEN_TEAMS = 2.0  # seconds between processing different teams\n",
    "MAX_RETRIES = 3  # number of retries on API failure\n",
    "RETRY_DELAY = 5.0  # seconds to wait before retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd640a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Configuration:\n",
      "   Teams to evaluate: 4\n",
      "   Criteria per team: 5\n",
      "   Delay between API calls: 1.0s\n",
      "   Delay between teams: 2.0s\n",
      "\n",
      "‚è±Ô∏è  Estimated runtime: 28 seconds (~0.5 minutes)\n",
      "   (Actual time may vary due to API response times and retries)\n",
      "\n",
      "üí° Tip: You can adjust rate limiting parameters in the configuration cell above\n"
     ]
    }
   ],
   "source": [
    "# Calculate expected runtime\n",
    "num_teams = len(pitch_decks_df)\n",
    "num_criteria = 5\n",
    "time_per_criterion = DELAY_BETWEEN_CALLS  # seconds\n",
    "time_per_team = (num_criteria * time_per_criterion) + DELAY_BETWEEN_TEAMS\n",
    "estimated_total_time = num_teams * time_per_team\n",
    "\n",
    "print(f\"üìä Evaluation Configuration:\")\n",
    "print(f\"   Teams to evaluate: {num_teams}\")\n",
    "print(f\"   Criteria per team: {num_criteria}\")\n",
    "print(f\"   Delay between API calls: {DELAY_BETWEEN_CALLS}s\")\n",
    "print(f\"   Delay between teams: {DELAY_BETWEEN_TEAMS}s\")\n",
    "print(f\"\\n‚è±Ô∏è  Estimated runtime: {estimated_total_time:.0f} seconds (~{estimated_total_time/60:.1f} minutes)\")\n",
    "print(f\"   (Actual time may vary due to API response times and retries)\")\n",
    "print(f\"\\nüí° Tip: You can adjust rate limiting parameters in the configuration cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1767427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grade_from_response(res):\n",
    "    '''Extract the word score (Excellent, Good, etc.) from a single response'''\n",
    "    try:\n",
    "        word_score = res.text.split(\"Category:\")[1].split(\"\\n\")[0].strip()\n",
    "        return word_score\n",
    "    except (IndexError, AttributeError) as e:\n",
    "        print(f\"Error extracting grade: {e}\")\n",
    "        print(f\"Response text: {res.text[:200]}\")\n",
    "        return \"Fair\"  # Default fallback score\n",
    "\n",
    "def get_token_counts(res):\n",
    "    '''Extract token counts from a response'''\n",
    "    prompt_tokens = res.usage_metadata.prompt_token_count if res.usage_metadata.prompt_token_count else 0\n",
    "    cand_tokens = res.usage_metadata.candidates_token_count if res.usage_metadata.candidates_token_count else 0\n",
    "    thought_tokens = res.usage_metadata.thoughts_token_count if res.usage_metadata.thoughts_token_count else 0\n",
    "    \n",
    "    return prompt_tokens, cand_tokens, thought_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f311772",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_criterion_with_retry(client, prompt, criterion_name, max_retries=MAX_RETRIES):\n",
    "    '''\n",
    "    Evaluate a single criterion with retry logic for robustness.\n",
    "    Uses separate API call for each criterion (no context maintenance).\n",
    "    '''\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Create a fresh chat session for this criterion\n",
    "            chat = client.aio.chats.create(model='gemini-2.5-flash-preview-09-2025')\n",
    "            response = await chat.send_message(prompt)\n",
    "            \n",
    "            # Add delay to respect rate limits\n",
    "            await asyncio.sleep(DELAY_BETWEEN_CALLS)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error evaluating {criterion_name} (attempt {attempt + 1}/{max_retries}): {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  ‚è≥ Retrying in {RETRY_DELAY} seconds...\")\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(f\"  ‚ùå Failed after {max_retries} attempts\")\n",
    "                raise\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "830238f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation of 4 teams using separate prompts\n",
      "‚è±Ô∏è  Start time: 09:57:46\n",
      "‚öôÔ∏è  Rate limiting: 1.0s between calls, 2.0s between teams\n",
      "\n",
      "============================================================\n",
      "üìä Processing Team 1/4: AquaSmart Innovations\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "     ‚úì Score: Good\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7195\n",
      "  ‚úÖ Team 1 completed successfully\n",
      "\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7195\n",
      "  ‚úÖ Team 1 completed successfully\n",
      "\n",
      "============================================================\n",
      "üìä Processing Team 2/4: Triad_Kernals_Problem_Deck_2025 - Inchara K Kuppal\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "============================================================\n",
      "üìä Processing Team 2/4: Triad_Kernals_Problem_Deck_2025 - Inchara K Kuppal\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "     ‚úì Score: Excellent\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Excellent\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 8363\n",
      "  ‚úÖ Team 2 completed successfully\n",
      "\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 8363\n",
      "  ‚úÖ Team 2 completed successfully\n",
      "\n",
      "============================================================\n",
      "üìä Processing Team 3/4: AgriSat Tech\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "============================================================\n",
      "üìä Processing Team 3/4: AgriSat Tech\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "     ‚úì Score: Fair\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Fair\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7018\n",
      "  ‚úÖ Team 3 completed successfully\n",
      "\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7018\n",
      "  ‚úÖ Team 3 completed successfully\n",
      "\n",
      "============================================================\n",
      "üìä Processing Team 4/4: RuralConnect\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "============================================================\n",
      "üìä Processing Team 4/4: RuralConnect\n",
      "============================================================\n",
      "  üìù Evaluating Problem Evidence...\n",
      "     ‚úì Score: Good\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üíº Evaluating Market Opportunity...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Good\n",
      "  üéØ Evaluating TIPSC...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üí° Evaluating Solution Direction...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üé® Evaluating Presentation...\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7204\n",
      "  ‚úÖ Team 4 completed successfully\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚ú® Evaluation complete!\n",
      "‚è±Ô∏è  Total time: 132.57 seconds (2.21 minutes)\n",
      "üìä Teams processed: 4/4\n",
      "üî¢ Total tokens used: 29780\n",
      "============================================================\n",
      "     ‚úì Score: Excellent\n",
      "  üìà Total tokens: 7204\n",
      "  ‚úÖ Team 4 completed successfully\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚ú® Evaluation complete!\n",
      "‚è±Ô∏è  Total time: 132.57 seconds (2.21 minutes)\n",
      "üìä Teams processed: 4/4\n",
      "üî¢ Total tokens used: 29780\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop using separate prompts for each criterion\n",
    "# Each criterion gets an independent API call (no context sharing)\n",
    "\n",
    "total_teams = len(pitch_decks_df)\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"üöÄ Starting evaluation of {total_teams} teams using separate prompts\")\n",
    "print(f\"‚è±Ô∏è  Start time: {start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"‚öôÔ∏è  Rate limiting: {DELAY_BETWEEN_CALLS}s between calls, {DELAY_BETWEEN_TEAMS}s between teams\\n\")\n",
    "\n",
    "for teamNum, team in enumerate(pitch_decks_df.itertuples()):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üìä Processing Team {teamNum + 1}/{total_teams}: {team.Team_Name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    grade_df.at[teamNum, 'Team_Name'] = team.Team_Name\n",
    "    token_df.at[teamNum, 'Team_Name'] = team.Team_Name\n",
    "    \n",
    "    total_prompt_tokens = 0\n",
    "    total_cand_tokens = 0\n",
    "    total_thought_tokens = 0\n",
    "    \n",
    "    try:\n",
    "        # 1. PROBLEM EVIDENCE & VALIDATION\n",
    "        print(\"  üìù Evaluating Problem Evidence...\")\n",
    "        ps_raw = \"Core Problem Statement: \" + team.Problem_Statement.split(\"Core Problem Statement:\", 1)[1].strip() + \"\\n\"\n",
    "        ps_evidence = team.Problem_Evidence.split(\"Slide\", 1)[1].split(\":\", 1)[1].strip()\n",
    "        problem_statement_text = ps_raw + ps_evidence\n",
    "        \n",
    "        PE_res = await evaluate_criterion_with_retry(\n",
    "            client, \n",
    "            prompt_prob_evidence_val(problem_statement_text),\n",
    "            \"Problem Evidence\"\n",
    "        )\n",
    "        PE_word_score = extract_grade_from_response(PE_res)\n",
    "        grade_df.at[teamNum, 'Problem_Evidence'] = PE_word_score\n",
    "        p_tokens, c_tokens, t_tokens = get_token_counts(PE_res)\n",
    "        total_prompt_tokens += p_tokens\n",
    "        total_cand_tokens += c_tokens\n",
    "        total_thought_tokens += t_tokens\n",
    "        print(f\"     ‚úì Score: {PE_word_score}\")\n",
    "        \n",
    "        \n",
    "        # 2. MARKET OPPORTUNITY & VIABILITY\n",
    "        print(\"  üíº Evaluating Market Opportunity...\")\n",
    "        market_opportunity_viability_text = team.Market_Opportunity_Viability.split(\"Slide\", 1)[1].split(\":\", 1)[1].strip()\n",
    "        \n",
    "        MOV_res = await evaluate_criterion_with_retry(\n",
    "            client,\n",
    "            prompt_market_viability(market_opportunity_viability_text),\n",
    "            \"Market Opportunity\"\n",
    "        )\n",
    "        MOV_word_score = extract_grade_from_response(MOV_res)\n",
    "        grade_df.at[teamNum, 'Market_Opp_Viability'] = MOV_word_score\n",
    "        p_tokens, c_tokens, t_tokens = get_token_counts(MOV_res)\n",
    "        total_prompt_tokens += p_tokens\n",
    "        total_cand_tokens += c_tokens\n",
    "        total_thought_tokens += t_tokens\n",
    "        print(f\"     ‚úì Score: {MOV_word_score}\")\n",
    "        \n",
    "        \n",
    "        # 3. TIPSC\n",
    "        print(\"  üéØ Evaluating TIPSC...\")\n",
    "        tipsc_text = \"Timely: \" + team.TIPSC.split(\"Timely\", 1)[1].split(\":\", 1)[1].strip()\n",
    "        \n",
    "        TIPSC_res = await evaluate_criterion_with_retry(\n",
    "            client,\n",
    "            prompt_tipsc(tipsc_text),\n",
    "            \"TIPSC\"\n",
    "        )\n",
    "        TIPSC_word_score = extract_grade_from_response(TIPSC_res)\n",
    "        grade_df.at[teamNum, 'TIPSC'] = TIPSC_word_score\n",
    "        p_tokens, c_tokens, t_tokens = get_token_counts(TIPSC_res)\n",
    "        total_prompt_tokens += p_tokens\n",
    "        total_cand_tokens += c_tokens\n",
    "        total_thought_tokens += t_tokens\n",
    "        print(f\"     ‚úì Score: {TIPSC_word_score}\")\n",
    "        \n",
    "        \n",
    "        # 4. SOLUTION DIRECTION & VALUE PROPOSITION\n",
    "        print(\"  üí° Evaluating Solution Direction...\")\n",
    "        solution_value_prop = team.Solution_Hypothesis.split(\"Slide\", 1)[1].split(\":\", 1)[1].strip()\n",
    "        \n",
    "        sol_res = await evaluate_criterion_with_retry(\n",
    "            client,\n",
    "            prompt_solution(solution_value_prop),\n",
    "            \"Solution Direction\"\n",
    "        )\n",
    "        sol_word_score = extract_grade_from_response(sol_res)\n",
    "        grade_df.at[teamNum, 'Solution_Dir_Val_Prop'] = sol_word_score\n",
    "        p_tokens, c_tokens, t_tokens = get_token_counts(sol_res)\n",
    "        total_prompt_tokens += p_tokens\n",
    "        total_cand_tokens += c_tokens\n",
    "        total_thought_tokens += t_tokens\n",
    "        print(f\"     ‚úì Score: {sol_word_score}\")\n",
    "        \n",
    "        \n",
    "        # 5. PRESENTATION & COHESION\n",
    "        print(\"  üé® Evaluating Presentation...\")\n",
    "        presentation_cohesion = team.Problem_Statement_Cleaned\n",
    "        \n",
    "        cohesion_res = await evaluate_criterion_with_retry(\n",
    "            client,\n",
    "            prompt_pres_comp(presentation_cohesion),\n",
    "            \"Presentation\"\n",
    "        )\n",
    "        cohesion_word_score = extract_grade_from_response(cohesion_res)\n",
    "        grade_df.at[teamNum, 'Pres_Cohesion'] = cohesion_word_score\n",
    "        p_tokens, c_tokens, t_tokens = get_token_counts(cohesion_res)\n",
    "        total_prompt_tokens += p_tokens\n",
    "        total_cand_tokens += c_tokens\n",
    "        total_thought_tokens += t_tokens\n",
    "        print(f\"     ‚úì Score: {cohesion_word_score}\")\n",
    "        \n",
    "        \n",
    "        # Record token statistics\n",
    "        token_df.at[teamNum, 'Input_Tokens'] = total_prompt_tokens\n",
    "        token_df.at[teamNum, 'Candidate_Tokens'] = total_cand_tokens\n",
    "        token_df.at[teamNum, 'Thought_Tokens'] = total_thought_tokens\n",
    "        token_df.at[teamNum, 'Output_Tokens'] = total_cand_tokens + total_thought_tokens\n",
    "        token_df.at[teamNum, 'Total_Tokens'] = total_prompt_tokens + total_cand_tokens + total_thought_tokens\n",
    "        \n",
    "        print(f\"  üìà Total tokens: {token_df.at[teamNum, 'Total_Tokens']}\")\n",
    "        print(f\"  ‚úÖ Team {teamNum + 1} completed successfully\\n\")\n",
    "        \n",
    "        # Delay between teams to respect rate limits\n",
    "        if teamNum < total_teams - 1:\n",
    "            await asyncio.sleep(DELAY_BETWEEN_TEAMS)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing team {team.Team_Name}: {str(e)}\")\n",
    "        print(f\"  Skipping to next team...\\n\")\n",
    "        continue\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚ú® Evaluation complete!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
    "print(f\"üìä Teams processed: {len(grade_df)}/{total_teams}\")\n",
    "print(f\"üî¢ Total tokens used: {token_df['Total_Tokens'].sum()}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd1226a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_Name</th>\n",
       "      <th>Problem_Evidence</th>\n",
       "      <th>Market_Opp_Viability</th>\n",
       "      <th>TIPSC</th>\n",
       "      <th>Solution_Dir_Val_Prop</th>\n",
       "      <th>Pres_Cohesion</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaSmart Innovations</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AgriSat Tech</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RuralConnect</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Team_Name Problem_Evidence  \\\n",
       "0                              AquaSmart Innovations             Good   \n",
       "1  Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...        Excellent   \n",
       "2                                       AgriSat Tech             Fair   \n",
       "3                                       RuralConnect             Good   \n",
       "\n",
       "  Market_Opp_Viability      TIPSC Solution_Dir_Val_Prop Pres_Cohesion  \\\n",
       "0                 Good  Excellent             Excellent     Excellent   \n",
       "1                 Good  Excellent             Excellent     Excellent   \n",
       "2                 Good  Excellent             Excellent     Excellent   \n",
       "3                 Good  Excellent             Excellent     Excellent   \n",
       "\n",
       "  Final_Score  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19310bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Usage Summary:\n",
      "============================================================\n",
      "Total Input Tokens: 13540\n",
      "Total Output Tokens: 16240\n",
      "Total Tokens: 29780\n",
      "\n",
      "Average tokens per team: 7445.00\n",
      "Max tokens for a single team: 8363\n",
      "Min tokens for a single team: 7018\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display token usage statistics\n",
    "print(\"Token Usage Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Input Tokens: {token_df['Input_Tokens'].sum()}\")\n",
    "print(f\"Total Output Tokens: {token_df['Output_Tokens'].sum()}\")\n",
    "print(f\"Total Tokens: {token_df['Total_Tokens'].sum()}\")\n",
    "print(f\"\\nAverage tokens per team: {token_df['Total_Tokens'].mean():.2f}\")\n",
    "print(f\"Max tokens for a single team: {token_df['Total_Tokens'].max()}\")\n",
    "print(f\"Min tokens for a single team: {token_df['Total_Tokens'].min()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6be718f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_Name</th>\n",
       "      <th>Candidate_Tokens</th>\n",
       "      <th>Thought_Tokens</th>\n",
       "      <th>Input_Tokens</th>\n",
       "      <th>Output_Tokens</th>\n",
       "      <th>Total_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaSmart Innovations</td>\n",
       "      <td>501</td>\n",
       "      <td>3502</td>\n",
       "      <td>3192</td>\n",
       "      <td>4003</td>\n",
       "      <td>7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...</td>\n",
       "      <td>522</td>\n",
       "      <td>3885</td>\n",
       "      <td>3956</td>\n",
       "      <td>4407</td>\n",
       "      <td>8363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AgriSat Tech</td>\n",
       "      <td>500</td>\n",
       "      <td>3329</td>\n",
       "      <td>3189</td>\n",
       "      <td>3829</td>\n",
       "      <td>7018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RuralConnect</td>\n",
       "      <td>480</td>\n",
       "      <td>3521</td>\n",
       "      <td>3203</td>\n",
       "      <td>4001</td>\n",
       "      <td>7204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Team_Name Candidate_Tokens  \\\n",
       "0                              AquaSmart Innovations              501   \n",
       "1  Triad_Kernals_Problem_Deck_2025 - Inchara K Ku...              522   \n",
       "2                                       AgriSat Tech              500   \n",
       "3                                       RuralConnect              480   \n",
       "\n",
       "  Thought_Tokens Input_Tokens Output_Tokens Total_Tokens  \n",
       "0           3502         3192          4003         7195  \n",
       "1           3885         3956          4407         8363  \n",
       "2           3329         3189          3829         7018  \n",
       "3           3521         3203          4001         7204  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6c427c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_Name</th>\n",
       "      <th>Problem_Evidence</th>\n",
       "      <th>Market_Opp_Viability</th>\n",
       "      <th>TIPSC</th>\n",
       "      <th>Solution_Dir_Val_Prop</th>\n",
       "      <th>Pres_Cohesion</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaSmart Innovations</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AgriSat Tech</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RuralConnect</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Team_Name Problem_Evidence Market_Opp_Viability      TIPSC  \\\n",
       "0  AquaSmart Innovations             Good                 Good  Excellent   \n",
       "2           AgriSat Tech             Fair                 Good  Excellent   \n",
       "3           RuralConnect             Good                 Good  Excellent   \n",
       "\n",
       "  Solution_Dir_Val_Prop Pres_Cohesion Final_Score  \n",
       "0             Excellent     Excellent         NaN  \n",
       "2             Excellent     Excellent         NaN  \n",
       "3             Excellent     Excellent         NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_df = grade_df.drop(grade_df.index[1])\n",
    "grade_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d9fdb",
   "metadata": {},
   "source": [
    "### Get final scores for each idea using weightages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eaa432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Score to Decimal Score mapping\n",
    "score_map = {'Poor' : 0.25,\n",
    "             'Fair' : 0.5,\n",
    "             'Good' : 0.75,\n",
    "             'Excellent' : 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba81859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for teamNum, team in enumerate(grade_df.itertuples()) :\n",
    "    grade_df.at[teamNum, 'Final_Score'] = 0.3 * score_map[team.Problem_Evidence] + \\\n",
    "                                        0.2 * score_map[team.Market_Opp_Viability] + \\\n",
    "                                        0.15 * score_map[team.TIPSC] + \\\n",
    "                                        0.15 * score_map[team.Solution_Dir_Val_Prop] + \\\n",
    "                                        0.2 * score_map[team.Pres_Cohesion]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d31f488b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_Name</th>\n",
       "      <th>Problem_Evidence</th>\n",
       "      <th>Market_Opp_Viability</th>\n",
       "      <th>TIPSC</th>\n",
       "      <th>Solution_Dir_Val_Prop</th>\n",
       "      <th>Pres_Cohesion</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaSmart Innovations</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AgriSat Tech</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RuralConnect</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Team_Name Problem_Evidence Market_Opp_Viability      TIPSC  \\\n",
       "0  AquaSmart Innovations             Good                 Good  Excellent   \n",
       "2           AgriSat Tech             Fair                 Good  Excellent   \n",
       "3           RuralConnect             Good                 Good  Excellent   \n",
       "1                    NaN              NaN                  NaN        NaN   \n",
       "\n",
       "  Solution_Dir_Val_Prop Pres_Cohesion Final_Score  \n",
       "0             Excellent     Excellent       0.875  \n",
       "2             Excellent     Excellent       0.875  \n",
       "3             Excellent     Excellent         NaN  \n",
       "1                   NaN           NaN         0.8  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cfe1d",
   "metadata": {},
   "source": [
    "### Implementation Comparison: Multi-turn vs Separate Prompts\n",
    "\n",
    "**Multi-turn approach (RateRealSolutions.ipynb):**\n",
    "- ‚úÖ Maintains context across all 5 criteria\n",
    "- ‚úÖ May provide more coherent evaluations\n",
    "- ‚úÖ Potentially fewer total tokens (shared context)\n",
    "- ‚ùå Single chat session = single point of failure\n",
    "- ‚ùå If one criterion fails, affects subsequent ones\n",
    "\n",
    "**Separate prompts approach (this notebook):**\n",
    "- ‚úÖ Independent evaluation for each criterion\n",
    "- ‚úÖ More resilient - failures are isolated\n",
    "- ‚úÖ Better rate limiting control\n",
    "- ‚úÖ Can parallelize if needed (future enhancement)\n",
    "- ‚úÖ Explicit retry logic per criterion\n",
    "- ‚ùå No context sharing between criteria\n",
    "- ‚ùå May use more tokens (repeated context)\n",
    "\n",
    "**Rate Limiting Strategy:**\n",
    "- `{DELAY_BETWEEN_CALLS}s` delay between each API call\n",
    "- `{DELAY_BETWEEN_TEAMS}s` delay between processing teams\n",
    "- Retry logic with exponential backoff\n",
    "- Progress tracking for transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9193f7",
   "metadata": {},
   "source": [
    "### Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e07aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved:\n",
      "   - alt_evaluation_results_20251028_095958.csv\n",
      "   - alt_token_usage_20251028_095958.csv\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "grade_df.to_csv(f'alt_evaluation_results_{timestamp}.csv', index=False)\n",
    "token_df.to_csv(f'alt_token_usage_{timestamp}.csv', index=False)\n",
    "print(f\"‚úÖ Results saved:\")\n",
    "print(f\"   - alt_evaluation_results_{timestamp}.csv\")\n",
    "print(f\"   - alt_token_usage_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f8841",
   "metadata": {},
   "source": [
    "---\n",
    "### üìö When to Use Which Approach?\n",
    "\n",
    "**Use Multi-turn Chat (RateRealSolutions.ipynb) when:**\n",
    "- You have generous API rate limits\n",
    "- Context between criteria is important for evaluation consistency\n",
    "- You want to minimize token usage\n",
    "- You need faster execution time\n",
    "- Your dataset is small to medium sized\n",
    "\n",
    "**Use Separate Prompts (this notebook) when:**\n",
    "- You have strict API rate limits\n",
    "- You need maximum reliability and fault tolerance\n",
    "- You want independent, unbiased evaluations per criterion\n",
    "- You're dealing with large datasets\n",
    "- You need detailed progress tracking and error handling\n",
    "- You want to experiment with different evaluation strategies per criterion\n",
    "\n",
    "**Performance Comparison:**\n",
    "\n",
    "| Metric | Multi-turn | Separate Prompts |\n",
    "|--------|-----------|------------------|\n",
    "| Context Sharing | ‚úÖ Yes | ‚ùå No |\n",
    "| Fault Tolerance | ‚ö†Ô∏è Medium | ‚úÖ High |\n",
    "| Rate Limit Control | ‚ö†Ô∏è Limited | ‚úÖ Excellent |\n",
    "| Token Efficiency | ‚úÖ Higher | ‚ö†Ô∏è Lower |\n",
    "| Execution Speed | ‚úÖ Faster | ‚ö†Ô∏è Slower |\n",
    "| Progress Tracking | ‚ö†Ô∏è Basic | ‚úÖ Detailed |\n",
    "| Retry Logic | ‚ùå No | ‚úÖ Yes |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIEEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
